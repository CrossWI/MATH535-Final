{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynTro-0t8SvV"
      },
      "source": [
        "# Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVX_51Fy8SvX"
      },
      "source": [
        "\n",
        "PCA is a commonly used dimensionality reduction technique that seeks to maximize the sample variance of the centered and normalized data (formula from pg 23 of 2.6.2.5 in latex) where $X$ denotes the data matrix, and $\\phi_1$ is the vector of loadings for the first principal component. A mathematically equivalent formulation is given by the eigenvectors of the covariance matrix $X^TX$, with the eigenvector associated with the largest eigenvalue corresponding to the first principal component. The process projects the data onto a lower dimension linear subspace, such that the data maintains as much of its original information as possible.\n",
        "\n",
        "\n",
        "An issue with utilizing standard PCA arises when considering linear separability. Singular value decomposition is a linear transformation, and as such, PCA does not work well when subjected to data with nonlinear decision boundaries.\n",
        "\n",
        "\n",
        "Kernel PCA is actually a more general case of standard PCA, and can generally handle nonlinear cases. The idea is to implement a nonlinear kernel function to project the data into a larger dimensional feature space where the data is then linearly separable, and then perform PCA. (figure 12.16 pg 587 microsoft).\n",
        "\n",
        "\n",
        "The MNIST dataset, as we have previously discussed, is a widely utilized set for training machine learning models. The set of handwritten digits consists of 60,000 training images, and 10,000 testing images, all of which are normalized according to size and grayscale levels. In this set in particular, the data is confined to a nonlinear subspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3e0WJ7o8SvX"
      },
      "source": [
        "# Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxMu33IH8SvX"
      },
      "source": [
        "First, we assume we have a nonlinear transformation $\\phi(x): D \\rightarrow M$ with $D >> M$. Each vector in the entry matrix $A$ is then projected using $\\phi(x_i)$ for all $x_i \\in A$. Standard PCA can be used in this new feature space, but it is costly and inefficient. This process can be simplified using kernel methods. We define our kernel method as:\n",
        "\n",
        "$$\n",
        "\\kappa(x_i, x_j) = exp(-\\frac{\\lVert x - y \\rVert^2}{2\\sigma^2}) = \\phi(x_i)^T\\phi(x_j)\n",
        "$$\n",
        "\n",
        "Generally speaking, we cannot assume that the projected features have zero mean. Thus we have to centralize the data:\n",
        "\n",
        "$$\n",
        "\\widetilde\\phi(x_n) = \\phi(x_n) - \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\n",
        "$$\n",
        "\n",
        "The elements of the Gram matrix are then\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\widetilde{K}_{nm} & = \\widetilde\\phi(x_n)^T\\widetilde\\phi(x_m)\\\\\n",
        "        & = \\phi(x_n)^T\\phi(x_m) - \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_n)^T\\phi(x_i) - \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)^T\\phi(x_m) + \\frac{1}{N^2}\\sum\\limits_{j=1}^N\\sum\\limits_{i=1}^N \\phi(x_j)^T\\phi(x_i)\\\\\n",
        "        & = k(x_n,x_m) - \\frac{1}{N}\\sum_{i=1}^{N} k(x_i,x_m) - \\frac{1}{N}\\sum_{i=1}^{N} k(x_n,x_i) + \\frac{1}{N^2}\\sum\\limits_{j=1}^N\\sum\\limits_{i=1}^N k(x_j,x_i)\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Rewriting in matrix notation yields\n",
        "\n",
        "$$\n",
        "\\widetilde{\\bf K} = {\\bf K - 1_{N}K - K1_{N} + 1_{N}K1_{N}}\n",
        "$$\n",
        "\n",
        "Where, by convention, ${\\bf 1_{N}}$ is the $N \\times N$ matrix with $1/N$ as entries. Using $\\widetilde{\\bf K}$ in place of ${\\bf K}$ will ensure the projected features have zero mean. Now, we construct the $M$ x $M$ covarience matrix of the projected features using\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\\phi(x_i)^T\n",
        "$$\n",
        "\n",
        "The eigenvalues and eigenvectors of $C$ are given by\n",
        "\n",
        "$$\n",
        "Cv_k = \\lambda_{k}v_k \\space\\space\\space \\forall \\space 1 \\leq k \\leq M\n",
        "$$\n",
        "\n",
        "It follows that\n",
        "\n",
        "$$\n",
        "(\\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\\phi(x_i)^T)v_k = \\lambda_{k}v_k\n",
        "$$\n",
        "\n",
        "Let $a_k$ be a principal component in $\\mathbb{R}^d$, then $v_k = \\sum_{i=1}^{N}a_{k,i}\\phi(x_i)$. Substituting this into the previous equation gives\n",
        "\n",
        "$$\n",
        "(\\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\\phi(x_i)^T)\\sum_{j=1}^{N}a_{k,j}\\phi(x_j) = \\lambda_{k}\\sum_{j=1}^{N}a_{k,j}\\phi(x_j)\n",
        "$$\n",
        "\n",
        "Now we define the kernel function as $\\kappa(x_i, x_j) = \\phi(x_i)^T\\phi(x_j)$. If we multiply the eigenvector equation on both sides by $\\phi(x_l)^T$, then we get\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)^T\\phi(x_i)\\phi(x_i)^T\\sum_{j=1}^{N}a_{k,j}\\phi(x_j) = \\lambda_{k}\\sum_{j=1}^{N}a_{k,j}\\phi(x_l)^T\\phi(x_j) & \\implies \\sum_{i=1}^{N}\\kappa(x_l, x_i)\\sum_{j=1}^{N}a_{k,j}\\kappa(x_i, x_j) = N\\lambda_{k}\\sum_{j=1}^{N}a_{k,j}\\kappa(x_l, x_j)\\\\\n",
        "    & \\implies K^2a_k = N\\lambda_{k}Ka_k\\\\\n",
        "    & \\implies Ka_k = N\\lambda_{k}a_k\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Thus, the kernel principal components are found using:\n",
        "\n",
        "$$\n",
        "y_k(x) = \\phi(x)^Tv_k = \\sum_{j=1}^{N}a_{k,j}\\kappa(x, x_j)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_NjLLf28SvY"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0zaYXVq8SvY"
      },
      "source": [
        "Before we start implementing kernel PCA in Python, we need to import the neccessary libraries and set up Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "viDjU0Vl8SvY"
      },
      "outputs": [],
      "source": [
        "import numpy as np              # used for linear algebra methods\n",
        "import matplotlib.pyplot as plt # used for plotting results \n",
        "import tensorflow as tf         # used to access the MNIST dataset\n",
        "from tensorflow import keras    # used to access the MNIST dataset\n",
        "import pandas as pd             # used to convert principal components to a data frame\n",
        "import seaborn as sns           # used for graphing results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0V6cONA8SvY"
      },
      "source": [
        "Kernel PCA consists of four steps:\n",
        "1. Constructing the kernel matrix $K$ from the training data set\n",
        "2. Centralizing $K$ by computing the Gram matrix $\\widetilde{K}$\n",
        "3. Computing the kernel principal components $y_k(x)$\n",
        "\n",
        "Step 1: We create a function that takes in a training data set and sigma value and constructs the Gaussian kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tNrK2diG8SvZ"
      },
      "outputs": [],
      "source": [
        "# Create Gaussian Kernel\n",
        "def gaus_kernel(X, sigma):\n",
        "\trow, col = X.shape\n",
        "\n",
        "\tK = np.zeros([row, row])\n",
        "\n",
        "\tfor i in range(0,row):\n",
        "\t\tfor j in range(0,row):\n",
        "\t\t\tv_i = X[i]\n",
        "\t\t\tv_j = X[j]\n",
        "\t\t\tK[i,j] = np.exp(-np.linalg.norm(v_i.T - v_j.T)**2/(2 * np.power(sigma, 2)))\n",
        "\n",
        "\treturn K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgK3fdoM8SvZ"
      },
      "source": [
        "Step 2: We create a function to compute the Gram matrix to ensure the projected features are centralized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4Dbg6FF_8SvZ"
      },
      "outputs": [],
      "source": [
        "# Construct the Gram Matrix\n",
        "def gram_matrix(K):\n",
        "\trow, col = K.shape\n",
        "\n",
        "\tone_N = np.ones((row, col)) / row\n",
        "\tK_tilde = K - (one_N @ K) - (K @ one_N) + (one_N @ K @ one_N)\n",
        "\treturn K_tilde"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5bcDqZT8SvZ"
      },
      "source": [
        "Step 3: Compute the kernel principal components $y_k(x)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gIAYs3Hi8SvZ"
      },
      "outputs": [],
      "source": [
        "# Compute the kernel principal components\n",
        "def compute_principal_components(K_tilde):\n",
        "  eig_vals, eig_vectors = np.linalg.eigh(K_tilde)\n",
        "  eig_vals, eig_vectors = eig_vals[::-1], eig_vectors[:, ::-1]\n",
        "\n",
        "  return np.column_stack([eig_vectors[:, i] for i in range(len(eig_vectors))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtQP3W508SvZ"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using Kernel PCA to differentiate between handwritten zeros and ones using the first two principal components. The handwritten numbers are obtained from the MNIST dataset, which is a large database of handwritten digits used for training various image processing systems and machine learning models. Before we work with the dataset, we want to load the data in and convert it into matrices."
      ],
      "metadata": {
        "id": "Gxd0VoC4Zxr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "q-bWBm-l8Sva"
      },
      "outputs": [],
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(train_imgs, train_labels), (test_imgs, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the first image and its corresponding label from the training dataset:"
      ],
      "metadata": {
        "id": "KJSia6jta9hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_imgs[0])\n",
        "plt.show()\n",
        "\n",
        "print(train_labels[0])"
      ],
      "metadata": {
        "id": "LU8RlkbNa930",
        "outputId": "9eb09981-4cfa-4708-fab9-25e89607c470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1zEougs8Sva"
      },
      "source": [
        "Instead of performing PCA on all of the data, we will look at the zeros and ones specifically. We will need to filter the data accordingly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i01 = [i for i in range(len(train_labels)) if (train_labels[i]==0) or (train_labels[i]==1)]\n",
        "train_imgs01 = train_imgs[i01]\n",
        "train_labels01 = train_labels[i01]"
      ],
      "metadata": {
        "id": "aulrGa1BbnqR"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first sample in the subset is:"
      ],
      "metadata": {
        "id": "fp-vshZpbwWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_imgs01[0])\n",
        "plt.show()\n",
        "\n",
        "print(train_labels01[0])"
      ],
      "metadata": {
        "id": "YLwJZNy-b0fD",
        "outputId": "8d863a95-2a7c-4867-b5bc-63216a4262ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to transform the samples into vectors:"
      ],
      "metadata": {
        "id": "_saa3ueBcBGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "NfVtnlak8Sva"
      },
      "outputs": [],
      "source": [
        "X = np.vstack([train_imgs01[i].flatten() for i in range(len(train_labels01))])\n",
        "y = train_labels01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we will run our kernel PCA implementation on the samples and graph the first two principle components."
      ],
      "metadata": {
        "id": "M52ivjKecNSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = gaus_kernel(X, 4000)\n",
        "K_tilde = gram_matrix(K)\n",
        "K_kpca = compute_principal_components(K_tilde)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df[\"y\"] = y\n",
        "df[\"comp-1\"] = K_kpca[:,0]\n",
        "df[\"comp-2\"] = K_kpca[:,1]\n",
        "\n",
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
        "                palette=sns.color_palette(\"hls\", 2),\n",
        "                data=df).set(title=\"Image KernelPCA projection\")"
      ],
      "metadata": {
        "id": "fnpraBSlcWs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** SPACE FOR OBSERVATIONS ***"
      ],
      "metadata": {
        "id": "wM_-9hnkcfP4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlSRUpEm8Sva"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chJFrOyS8Sva"
      },
      "source": [
        "- https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
        "- https://arxiv.org/pdf/1207.3538.pdf\n",
        "- https://en.wikipedia.org/wiki/Kernel_principal_component_analysis#:~:text=In%20the%20field%20of%20multivariate,a%20reproducing%20kernel%20Hilbert%20space\n",
        "- https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf\n",
        "- https://en.wikipedia.org/wiki/MNIST_database\n",
        "- https://people.math.wisc.edu/~roch/mmids/roch-mmids-opt-1motiv.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}