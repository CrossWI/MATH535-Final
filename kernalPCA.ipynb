{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA is a commonly used dimensionality reduction technique that seeks to maximize the sample variance of the centered and normalized data according to (formula from pg 23 of 2.6.2.5 in latex) where $X$ denotes the data matrix, and $\\phi_1$ is the vector of loadings for the first principal component. A mathematically equivalent formulation is given by the eigenvectors of the covariance matrix $X^TX$, with the eigenvector associated with the largest eigenvalue corresponding to the first principal component. The process projects the data onto a lower dimension linear subspace, such that the data maintains as much of its original information as possible.\n",
    "\n",
    "\n",
    "An issue with utilizing standard PCA arises when considering linear separability. Singular value decomposition is a linear transformation, and as such, PCA does not work well when subjected to data with nonlinear decision boundaries.\n",
    "\n",
    "\n",
    "Kernel PCA is actually a more general case of standard PCA, and can generally handle nonlinear cases. The idea is to implement a nonlinear kernel function to project the data into a larger dimensional feature space where the data is then linearly separable, and then perform PCA. (figure 12.16 pg 587 microsoft).\n",
    "\n",
    "\n",
    "The MNIST dataset, as we have previously discussed, is a widely utilized set for training machine learning models. The set of handwritten digits consists of 60,000 training images, and 10,000 testing images, all of which are normalized according to size and grayscale levels. In this set in particular, the data is confined to a nonlinear subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assume we have a nonlinear transformation $\\phi(x): D \\rightarrow M$ with $D >> M$. Each vector in the entry matrix $A$ is then projected using $\\phi(x_i)$ for all $x_i \\in A$. Standard PCA can be used in this new feature space, but it is costly and inefficient. This process can be simplified using kernel methods. We define our kernel method as:\n",
    "\n",
    "$$\n",
    "\\kappa(x_i, x_j) = exp(-\\frac{\\lVert x - y \\rVert^2}{2\\sigma^2}) = \\phi(x_i)^T\\phi(x_j)\n",
    "$$\n",
    "\n",
    "Generally speaking, we cannot assume that the projected features have zero mean. Thus we have to centralize the data:\n",
    "\n",
    "$$\n",
    "\\widetilde\\phi(x_n) = \\phi(x_n) - \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\n",
    "$$\n",
    "\n",
    "The elements of the Gram matrix are then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\widetilde{K}_{nm} & = \\widetilde\\phi(x_n)^T\\widetilde\\phi(x_m)\\\\\n",
    "        & = \\phi(x_n)^T\\phi(x_m) - \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_n)^T\\phi(x_i) - \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)^T\\phi(x_m) + \\frac{1}{N^2}\\sum\\limits_{j=1}^N\\sum\\limits_{i=1}^N \\phi(x_j)^T\\phi(x_i)\\\\\n",
    "        & = k(x_n,x_m) - \\frac{1}{N}\\sum_{i=1}^{N} k(x_i,x_m) - \\frac{1}{N}\\sum_{i=1}^{N} k(x_n,x_i) + \\frac{1}{N^2}\\sum\\limits_{j=1}^N\\sum\\limits_{i=1}^N k(x_j,x_i)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Rewriting in matrix notation yields\n",
    "\n",
    "$$\n",
    "\\widetilde{\\bf K} = {\\bf K - 1_{\\textnormal N}K - K1_{\\textnormal N} + 1_{\\textnormal N}K1_{\\textnormal N}}\n",
    "$$\n",
    "\n",
    "Where, by convention, ${\\bf 1_{\\textnormal N}}$ is the $N \\times N$ matrix with $1/N$ as entries. Using $\\widetilde{\\bf K}$ in place of ${\\bf K}$ will ensure the projected features have zero mean. Now, we construct the $M$ x $M$ covarience matrix of the projected features using\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\\phi(x_i)^T\n",
    "$$\n",
    "\n",
    "The eigenvalues and eigenvectors of $C$ are given by\n",
    "\n",
    "$$\n",
    "Cv_k = \\lambda_{k}v_k \\space\\space\\space \\forall \\space 1 \\leq k \\leq M\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "(\\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\\phi(x_i)^T)v_k = \\lambda_{k}v_k\n",
    "$$\n",
    "\n",
    "Let $a_k$ be a principal component in $\\mathbb{R}^d$, then $v_k = \\sum_{i=1}^{N}a_{k,i}\\phi(x_i)$. Substituting this into the previous equation gives\n",
    "\n",
    "$$\n",
    "(\\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\\phi(x_i)^T)\\sum_{j=1}^{N}a_{k,j}\\phi(x_j) = \\lambda_{k}\\sum_{j=1}^{N}a_{k,j}\\phi(x_j)\n",
    "$$\n",
    "\n",
    "Now we define the kernel function as $\\kappa(x_i, x_j) = \\phi(x_i)^T\\phi(x_j)$. If we multiply the eigenvector equation on both sides by $\\phi(x_l)^T$, then we get\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)^T\\phi(x_i)\\phi(x_i)^T\\sum_{j=1}^{N}a_{k,j}\\phi(x_j) = \\lambda_{k}\\sum_{j=1}^{N}a_{k,j}\\phi(x_l)^T\\phi(x_j) & \\implies \\sum_{i=1}^{N}\\kappa(x_l, x_i)\\sum_{j=1}^{N}a_{k,j}\\kappa(x_i, x_j) = N\\lambda_{k}\\sum_{j=1}^{N}a_{k,j}\\kappa(x_l, x_j)\\\\\n",
    "    & \\implies K^2a_k = N\\lambda_{k}Ka_k\\\\\n",
    "    & \\implies Ka_k = N\\lambda_{k}a_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the kernel principal components are found using:\n",
    "\n",
    "$$\n",
    "y_k(x) = \\phi(x)^Tv_k = \\sum_{j=1}^{N}a_{k,j}\\kappa(x, x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start implementing kernel PCA in Python, we need to import the neccessary libraries and set up Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA consists of four steps:\n",
    "1. Constructing the kernel matrix $K$ from the training data set\n",
    "2. Centralizing $K$ by computing the Gram matrix $\\widetilde{K}$\n",
    "3. Solving for the vectors $a_i$\n",
    "4. Computing the kernel principal components $y_k(x)$\n",
    "\n",
    "Step 1: We create a function that takes in a training data set and sigma value and constructs the Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Kernel\n",
    "def gaus_kernel(X, sigma):\n",
    "\trow, col = X.shape\n",
    "\n",
    "\tK = np.zeros([row, row])\n",
    "\n",
    "\tfor i in range(0,row):\n",
    "\t\tfor j in range(0,row):\n",
    "\t\t\tv_i = X[i]\n",
    "\t\t\tv_j = X[j]\n",
    "\t\t\tK[i,j] = np.exp(-np.linalg.norm(v_i.T - v_j.T)**2/(2 * np.power(sigma, 2)))\n",
    "\n",
    "\treturn K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: We create a function to compute the Gram matrix to ensure the projected features are centralized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Gram Matrix\n",
    "def gram_matrix(K):\n",
    "\trow, col = K.shape\n",
    "\n",
    "\tone_N = np.ones((row, col)) / row\n",
    "\tK_tilde = K - (one_N @ K) - (K @ one_N) + (one_N @ K @ one_N)\n",
    "\treturn K_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Now that we have the Gram matrix, we need to solve for the vectors $a_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for the eigenvectors a_i\n",
    "def solve_ai(K_tilde):\n",
    "\teig_vals, eig_vectors = np.linalg.eigh(K_tilde)\n",
    "\teig_vals, eig_vectors = eig_vals[::-1], eig_vectors[:, ::-1]\n",
    "\treturn eig_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compute the kernel principal components $y_k(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the kernel principal components\n",
    "def compute_kernel_components(eigvecs):\n",
    "    y = []\n",
    "    for a_k in eigvecs:\n",
    "        y_k = np.zeros(len(a_k), 1)\n",
    "        for x in X:\n",
    "            for i in range(0, len(X)):\n",
    "                k = np.exp(-np.linalg.norm(x.T - x[i].T)**2/(2 * np.power(sigma, 2)))\n",
    "                y_k += a_k[i] * k\n",
    "            y.append(y_k)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created functions for the four steps of kernel PCA, we can apply them to the MNIST dataset, but first we must get the training data set and filter it to be only zeros and ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_imgs, train_labels), (test_imgs, test_labels) = mnist.load_data()\n",
    "\n",
    "i01 = [i for i in range(len(train_labels[0:100])) if (train_labels[i]==0) or (train_labels[i]==1)]\n",
    "imgs01 = train_imgs[i01]\n",
    "labels01 = train_labels[i01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform kernel PCA on the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack([imgs01[i].flatten() for i in range(len(labels01))])\n",
    "y = labels01\n",
    "\n",
    "K = gaus_kernel(X, 20)\n",
    "K_tilde = gram_matrix(K)\n",
    "eigvecs = solve_ai(K_tilde)\n",
    "K_kpca = compute_kernel_components(eigvecs)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = y\n",
    "df[\"comp-1\"] = X_kpca[:,0]\n",
    "df[\"comp-2\"] = X_kpca[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 3),\n",
    "                data=df).set(title=\"Image KernelPCA projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
    "- https://arxiv.org/pdf/1207.3538.pdf\n",
    "- https://en.wikipedia.org/wiki/Kernel_principal_component_analysis#:~:text=In%20the%20field%20of%20multivariate,a%20reproducing%20kernel%20Hilbert%20space\n",
    "- https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
